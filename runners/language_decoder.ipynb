{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# pretend to be in root dir\n",
    "os.chdir(\"..\")\n",
    "# add VideoLLaMA to path (for imports in VideoLLaMA)\n",
    "sys.path.append('VideoLLaMA')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.VideoLlamaAL import VideoLlamaAL\n",
    "from models.language_decoder_utils import load_qformer, save_qformer, get_imagebind_embeds\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.ImagebindEmbedsDataset import ImagebindEmbedsDataset\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = VideoLlamaAL()\n",
    "model = model.to(device)\n",
    "\n",
    "# load AL checkpoint\n",
    "ckpt = torch.load(\"../Video-LLaMA-2-7B-Finetuned/AL_LLaMA_2_7B_Finetuned.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt['model'], strict=False)\n",
    "\n",
    "print(model.generate_text_only(\"Hello?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_qformer(model, \"model_saves/language_decoder/ckpts/19.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on sensor stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "batch_size = 1\n",
    "num_iters = 0\n",
    "num_iters_to_eval = 50\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-6) # VideoLLaMA uses 1e-5 with batch size of 4, 1e-5/4 = 2.5e-6\n",
    "for i in range(20):\n",
    "    dataloader = DataLoader(ImagebindEmbedsDataset(\"data/sensor_embeddings_random_8_new\", \"data/actionsense_processed\", \"data/train_random_8.csv\"), batch_size, shuffle=True)\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for imagebind_embeds, captions in pbar:\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        # imagebind_embeds += torch.normal(0, 0.01, size=imagebind_embeds.shape, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(imagebind_embeds, captions)['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        num_iters += 1\n",
    "        if num_iters % num_iters_to_eval == 0:\n",
    "            pbar.set_description(f\"Loss: {total_loss / num_iters_to_eval}, {model.generate(imagebind_embeds)}\")\n",
    "            total_loss = 0\n",
    "    \n",
    "    save_qformer(model, f\"model_saves/language_decoder/nonoise_ckpts/{i}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Llava instruct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from dataset.LlavaDataset import LlavaDataset\n",
    "\n",
    "batch_size = 1\n",
    "num_iters = 0\n",
    "for i in range(1):\n",
    "    dataloader = DataLoader(LlavaDataset(\"data/llava_instruct_150k.json\", \"data/train2017_embeds\"), batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for imagebind_embeds, prompts, targets in pbar:\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(imagebind_embeds, targets, prompt=prompts[0], num_patch_tokens=1)['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_iters += 1\n",
    "\n",
    "        num_iters_to_eval = 50\n",
    "        if num_iters % num_iters_to_eval == 0:\n",
    "            pbar.set_description(f\"Loss: {total_loss / num_iters_to_eval}\")\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"evaluation/eval_prompts.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "for entry in json_data:\n",
    "    subject = entry[\"subject\"]\n",
    "    start = entry[\"start\"]\n",
    "    prompt = entry[\"prompt\"]\n",
    "    print(subject, start, prompt)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(model.generate(\n",
    "        get_imagebind_embeds(f\"data/sensor_embeddings_random_8/{subject}\", start, start+7), \n",
    "        prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere>. {prompt}\\n###\\nAssistant:\\n\"\n",
    "    ))\n",
    "    \"\"\"\n",
    "\n",
    "    print(model.generate(\n",
    "        get_imagebind_embeds(f\"data/sensor_embeddings_random_8/{subject}\", start, start+0),\n",
    "        prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere>. {prompt}\\n###\\nAssistant:\\n\",\n",
    "        num_patch_tokens=1,\n",
    "    ))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(\n",
    "    get_imagebind_embeds(f\"data/imagebind_targets_dark/S09\", 1495, 1495),\n",
    "    prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere>. Describe what is happening in this scene.\\n###\\nAssistant:\\n\",\n",
    "    num_patch_tokens=1,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from evaluation.caption_metrics import calculate_metrics_new\n",
    "\n",
    "def generate_eval_sample(dataset, prompt_type=\"regular\", prompt=None, num=100):\n",
    "    outputs = []\n",
    "    captions_list = []\n",
    "    for imagebind_embeds, captions in tqdm(islice(DataLoader(dataset, batch_size=1, shuffle=True), num)):\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        outputs.append(model.generate(imagebind_embeds, prompt_type=prompt_type, prompt=prompt))\n",
    "        # outputs.append(model.generate(imagebind_embeds[:,0:1,:], prompt_type=prompt_type, num_patch_tokens=1))\n",
    "        captions_list.append(captions[0])\n",
    "\n",
    "    outputs = [output.replace(\"<s> \", \"\").replace(\"</s>\", \"\") for output in outputs]\n",
    "    captions_list = [caption.replace(\"</s>\", \"\") for caption in captions_list]\n",
    "    return outputs, captions_list\n",
    "\n",
    "# load_qformer(model, \"model_saves/language_decoder/ckpts/17.pth\")\n",
    "load_qformer(model, \"model_saves/language_decoder/videollama.pth\")\n",
    "\n",
    "# outputs, captions_list = generate_eval_sample(ImagebindEmbedsDataset(\"data/sensor_embeddings_random_8_new\", \"data/actionsense_processed\", \"data/test_random_8.csv\"), num=len(ImagebindEmbedsDataset(\"data/sensor_embeddings_random_8_new\", \"data/actionsense_processed\", \"data/test_random_8.csv\")))\n",
    "outputs, captions_list = generate_eval_sample(ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test_random_8.csv\"), num=len(ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test_random_8.csv\")))\n",
    "print(outputs[0])\n",
    "calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_8_dataset = ImagebindEmbedsDataset(\"data/sensor_embeddings_random_8\", \"data/actionsense_processed\", \"data/test_random_8.csv\")\n",
    "subject_split_dataset = ImagebindEmbedsDataset(\"data/sensor_embeddings\", \"data/actionsense_processed\", \"data/test.csv\")\n",
    "\n",
    "models_to_evaluate = [\n",
    "    # (\"regular\", \"model_saves/language_decoder/AL_26048stage1_listlabels_random_8.pth\", random_8_dataset, \"list labels\"),\n",
    "    # (\"regular 2\", \"model_saves/language_decoder/AL_6512stage1_random_8.pth\", random_8_dataset, \"regular\"),\n",
    "\n",
    "    # (\"unseen users\", \"model_saves/language_decoder/AL11_6826stage1_noise.pth\", subject_split_dataset, \"regular\"),\n",
    "\n",
    "    # (\"w/o noise\", \"model_saves/language_decoder/AL10_6826stage1.pth\", subject_split_dataset, \"regular\"),\n",
    "    # must do special run with 1 patch token only\n",
    "    # (\"w/o temporal embeddings\", \"model_saves/language_decoder/AL13_6826stage1_onepatch.pth\", subject_split_dataset, \"regular\"),\n",
    "\n",
    "    # (\"muscle only\", \"model_saves/language_decoder/AL_6826stage1_muscle.pth\", ImagebindEmbedsDataset(\"data/sensor_embeddings_muscle\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\"),\n",
    "    # (\"body only\", \"model_saves/language_decoder/AL_6826stage1_body.pth\", ImagebindEmbedsDataset(\"data/sensor_embeddings_body\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\"),\n",
    "\n",
    "    # (\"VideoLLaMA\", \"model_saves/language_decoder/videollama_6826stage1.pth\", ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\")\n",
    "]\n",
    "\n",
    "for name, qformer_path, dataset, prompt_type in models_to_evaluate:\n",
    "    print(\"=\" * 50)\n",
    "    print(name)\n",
    "    load_qformer(model, qformer_path)\n",
    "    outputs, captions_list = generate_eval_sample(dataset, prompt_type)\n",
    "    print(outputs[0])\n",
    "    calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-finetuned VideoLLaMA eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.caption_metrics import calculate_metrics_new\n",
    "from dataset.ImagebindEmbedsDataset import reworded_captions\n",
    "\n",
    "activity_labels = list(caption.replace(\"</s>\", \"\") for caption in reworded_captions.values())\n",
    "vanilla_prompt = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that can understand videos. The user will give you a video and ask you for a caption. You should choose one of the following captions to output:\\n\"\n",
    "for label in activity_labels:\n",
    "    vanilla_prompt += f\"- {label}\\n\"\n",
    "vanilla_prompt += f\"You should output a single caption and nothing more. You don't need to explain.\\n<</SYS>>\\n\\nOpen your eyes and imagine you see: {'<ImageHere>' * 8}. Give me a caption. Only output the caption; do not output anything else. [/INST]\\n\"\n",
    "print(vanilla_prompt)\n",
    "\n",
    "outputs, captions_list = generate_eval_sample(ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test_random_8_old.csv\"), prompt=vanilla_prompt, num=20)\n",
    "outputs = [output.replace(\"<s>\",\"\").replace(\"\\n\",\"\") for output in outputs]\n",
    "outputs = [\" \".join(output.split(\" \")[:30]) for output in outputs] # if response is too long, metrics will error; choose first 30 words\n",
    "print(outputs[0:5])\n",
    "calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_qformer(model, \"model_saves/language_decoder/regular.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "import csv\n",
    "from dataset.ImagebindEmbedsDataset import reworded_captions\n",
    "\n",
    "outputs = []\n",
    "captions_list = []\n",
    "# for imagebind_embeds, captions in tqdm(islice(DataLoader(dataset_test, batch_size=1, shuffle=True), 5)):\n",
    "for imagebind_embeds, captions in tqdm(DataLoader(dataset_test, batch_size=1, shuffle=True)):\n",
    "    imagebind_embeds = imagebind_embeds.to(device)\n",
    "    outputs.append(model.generate(imagebind_embeds).replace(\"<s> \", \"\").replace(\"</s>\", \"\"))\n",
    "    captions_list.append(captions[0].replace(\"<s> \", \"\").replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in islice(zip(outputs, captions_list), 5):\n",
    "    print(x)\n",
    "print(\"Accuracy: \", sum(output == caption for output, caption in zip(outputs, captions_list)))\n",
    "\n",
    "recognized_captions = list(caption.replace(\"</s>\", \"\") for caption in reworded_captions.values()) + [\"other\"]\n",
    "adjusted_outputs = [(output if output in recognized_captions else \"other\") for output in outputs]\n",
    "print(adjusted_outputs.count(\"other\"))\n",
    "confusion_matrix = [[0 for i in range(len(recognized_captions))] for j in range(len(recognized_captions))]\n",
    "for output, caption in zip(adjusted_outputs, captions_list):\n",
    "    confusion_matrix[recognized_captions.index(caption)][recognized_captions.index(output)] += 1\n",
    "\n",
    "pprint(confusion_matrix)\n",
    "\n",
    "with open('language_decoder/eval_results/confusion_matrix_big.csv', 'w', newline='') as file:\n",
    "   csv.writer(file).writerows(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_groups = [\n",
    "    [\n",
    "        'A person is cleaning a pan with a sponge.',\n",
    "        'A person is cleaning a pan with a towel.',\n",
    "        'A person is cleaning a plate with a sponge.',\n",
    "        'A person is cleaning a plate with a towel.',\n",
    "        'A person is clearing the cutting board.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is getting plates, bowls, mugs, glasses, and utensils from the cabinets.',\n",
    "        'A person is retrieving items from the refrigerator, cabinets, and drawers.',\n",
    "        'A person is replacing items in the refrigerator, cabinets, and drawers.',\n",
    "        'A person is loading the dishwasher with plates, bowls, mugs, glasses, and utensils.',\n",
    "        'A person is unloading the plates, bowls, mugs, glasses, and utensils from the dishwasher.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is putting plates and bowls onto the table.',\n",
    "        'A person is setting the table with plates, bowls, mugs, glasses, and utensils.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is opening a jar of almond butter.',\n",
    "        'A person is opening and closing a jar of almond butter.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is peeling a cucumber.',\n",
    "        'A person is peeling a potato.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is slicing a cucumber.',\n",
    "        'A person is slicing a potato.',\n",
    "        'A person is slicing the bread.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is spreading almond butter on a bread slice.',\n",
    "        'A person is spreading jelly on a bread slice.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is pouring water from a pitcher into a glass.',\n",
    "    ],\n",
    "    [\n",
    "        'other'\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group(x):\n",
    "    for i in range(len(caption_groups)):\n",
    "        if x in caption_groups[i]:\n",
    "            return i\n",
    "\n",
    "group_confusion_matrix = [[0 for i in range(len(caption_groups))] for j in range(len(caption_groups))]\n",
    "for output, caption in zip(adjusted_outputs, captions_list):\n",
    "    group_confusion_matrix[find_group(caption)][find_group(output)] += 1\n",
    "pprint(group_confusion_matrix)\n",
    "\n",
    "with open('language_decoder/eval_results/group_confusion_matrix_big.csv', 'w', newline='') as file:\n",
    "   csv.writer(file).writerows(group_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('videollama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12fc2016cfa40a85c17bb5faa80a8240ee193b1fac15980e1262bc37ef6d15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
