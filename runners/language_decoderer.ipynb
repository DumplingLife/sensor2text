{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/videollama/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ubuntu/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Overriding torch_dtype=torch.bfloat16 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: library_root already registered. ignoring error\n",
      "error: repo_root already registered. ignoring error\n",
      "error: cache_root already registered. ignoring error\n",
      "Loading LLAMA Tokenizer\n",
      "Loading LLAMA Model\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      " Hello?\n",
      "\n",
      "I'm just an AI, I don't have a physical body, but I'm here to help you with any questions or problems you might have. Is there something specific you'd like to talk about or ask?</s>\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# pretend to be in root dir\n",
    "os.chdir(\"..\")\n",
    "# add VideoLLaMA to path (for imports in VideoLLaMA)\n",
    "sys.path.append('VideoLLaMA')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.VideoLlamaAL import VideoLlamaAL\n",
    "from models.language_decoder_utils import load_qformer, save_qformer, get_imagebind_embeds\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.ImagebindEmbedsDataset import ImagebindEmbedsDataset\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = VideoLlamaAL()\n",
    "model = model.to(device)\n",
    "\n",
    "# load AL checkpoint\n",
    "ckpt = torch.load(\"../Video-LLaMA-2-7B-Finetuned/AL_LLaMA_2_7B_Finetuned.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt['model'], strict=False)\n",
    "\n",
    "print(model.generate_text_only(\"Hello?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_qformer(model, \"model_saves/language_decoder/ckpts/19.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on sensor stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "batch_size = 1\n",
    "num_iters = 0\n",
    "num_iters_to_eval = 50\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-6) # VideoLLaMA uses 1e-5 with batch size of 4, 1e-5/4 = 2.5e-6\n",
    "for i in range(20):\n",
    "    # dataloader = DataLoader(ImagebindEmbedsDataset(\"data/sensor_embeddings\", \"data/actionsense_processed\", \"data/train_random_8.csv\"), batch_size, shuffle=True)\n",
    "    dataloader = DataLoader(ImagebindEmbedsDataset(\"data/sensor_embeddings_body\", \"data/actionsense_processed\", \"data/train_random_8.csv\"), batch_size, shuffle=True)\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for imagebind_embeds, captions in pbar:\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        imagebind_embeds += torch.normal(0, 0.01, size=imagebind_embeds.shape, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(imagebind_embeds, captions)['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        num_iters += 1\n",
    "        if num_iters % num_iters_to_eval == 0:\n",
    "            pbar.set_description(f\"Loss: {total_loss / num_iters_to_eval}, {model.generate(imagebind_embeds)}\")\n",
    "            total_loss = 0\n",
    "    \n",
    "    save_qformer(model, f\"model_saves/language_decoder/body/{i}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Llava instruct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from dataset.LlavaDataset import LlavaDataset\n",
    "\n",
    "batch_size = 1\n",
    "num_iters = 0\n",
    "for i in range(1):\n",
    "    dataloader = DataLoader(LlavaDataset(\"data/llava_instruct_150k.json\", \"data/train2017_embeds\"), batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for imagebind_embeds, prompts, targets in pbar:\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(imagebind_embeds, targets, prompt=prompts[0], num_patch_tokens=1)['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_iters += 1\n",
    "\n",
    "        num_iters_to_eval = 50\n",
    "        if num_iters % num_iters_to_eval == 0:\n",
    "            pbar.set_description(f\"Loss: {total_loss / num_iters_to_eval}\")\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"evaluation/eval_prompts.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "for entry in json_data:\n",
    "    subject = entry[\"subject\"]\n",
    "    start = entry[\"start\"]\n",
    "    prompt = entry[\"prompt\"]\n",
    "    print(subject, start, prompt)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(model.generate(\n",
    "        get_imagebind_embeds(f\"data/sensor_embeddings_random_8/{subject}\", start, start+7), \n",
    "        prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere><ImageHere>. {prompt}\\n###\\nAssistant:\\n\"\n",
    "    ))\n",
    "    \"\"\"\n",
    "\n",
    "    print(model.generate(\n",
    "        get_imagebind_embeds(f\"data/sensor_embeddings_random_8/{subject}\", start, start+0),\n",
    "        prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere>. {prompt}\\n###\\nAssistant:\\n\",\n",
    "        num_patch_tokens=1,\n",
    "    ))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(\n",
    "    get_imagebind_embeds(f\"data/imagebind_targets_dark/S09\", 1495, 1495),\n",
    "    prompt=f\"<s>###\\nUser:\\nOpen your eyes and imagine you see: <ImageHere>. Describe what is happening in this scene.\\n###\\nAssistant:\\n\",\n",
    "    num_patch_tokens=1,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [07:05,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is retrieving items from the refrigerator, cabinets, and drawers.\n",
      "{'testlen': 1550, 'reflen': 1570, 'guess': [1550, 1387, 1224, 1061], 'correct': [1252, 1007, 816, 634]}\n",
      "ratio: 0.9872611464961865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/ubuntu/miniconda3/envs/videollama/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.128 s\n",
      "{'BLEU-1': 0.7973864123989484, 'BLEU-2': 0.7559777618840484, 'BLEU-3': 0.7218400824116402, 'BLEU-4': 0.6863146414923732, 'ROUGE': 0.8068079349841, 'METEOR': 0.4721846406203272, 'CIDER': 6.307592110233796, 'SPICE': 0.7117061143676083}\n",
      "accuracy:  93\n",
      "number of non-matching captions:  0\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from evaluation.caption_metrics import calculate_metrics_new\n",
    "\n",
    "def generate_eval_sample(dataset, prompt_type=\"regular\", prompt=None, num=None):\n",
    "    outputs = []\n",
    "    captions_list = []\n",
    "    if num is None:\n",
    "        num = len(dataset)\n",
    "    for imagebind_embeds, captions in tqdm(islice(DataLoader(dataset, batch_size=1, shuffle=True), num)):\n",
    "        imagebind_embeds = imagebind_embeds.to(device)\n",
    "        outputs.append(model.generate(imagebind_embeds, prompt_type=prompt_type, prompt=prompt))\n",
    "        # outputs.append(model.generate(imagebind_embeds[:,0:1,:], prompt_type=prompt_type, num_patch_tokens=1))\n",
    "        captions_list.append(captions[0])\n",
    "\n",
    "    outputs = [output.replace(\"<s> \", \"\").replace(\"</s>\", \"\") for output in outputs]\n",
    "    captions_list = [caption.replace(\"</s>\", \"\") for caption in captions_list]\n",
    "    return outputs, captions_list\n",
    "\n",
    "# load_qformer(model, \"model_saves/language_decoder/ckpts/17.pth\")\n",
    "load_qformer(model, \"model_saves/language_decoder/body_ckpts/19.pth\")\n",
    "outputs, captions_list = generate_eval_sample(ImagebindEmbedsDataset(\"data/sensor_embeddings\", \"data/actionsense_processed\", \"data/val_random_8.csv\"))\n",
    "print(outputs[0])\n",
    "calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_8_dataset = ImagebindEmbedsDataset(\"data/sensor_embeddings_random_8\", \"data/actionsense_processed\", \"data/test_random_8.csv\")\n",
    "subject_split_dataset = ImagebindEmbedsDataset(\"data/sensor_embeddings\", \"data/actionsense_processed\", \"data/test.csv\")\n",
    "\n",
    "models_to_evaluate = [\n",
    "    # (\"regular\", \"model_saves/language_decoder/AL_26048stage1_listlabels_random_8.pth\", random_8_dataset, \"list labels\"),\n",
    "    # (\"regular 2\", \"model_saves/language_decoder/AL_6512stage1_random_8.pth\", random_8_dataset, \"regular\"),\n",
    "\n",
    "    # (\"unseen users\", \"model_saves/language_decoder/AL11_6826stage1_noise.pth\", subject_split_dataset, \"regular\"),\n",
    "\n",
    "    # (\"w/o noise\", \"model_saves/language_decoder/AL10_6826stage1.pth\", subject_split_dataset, \"regular\"),\n",
    "    # must do special run with 1 patch token only\n",
    "    # (\"w/o temporal embeddings\", \"model_saves/language_decoder/AL13_6826stage1_onepatch.pth\", subject_split_dataset, \"regular\"),\n",
    "\n",
    "    # (\"muscle only\", \"model_saves/language_decoder/AL_6826stage1_muscle.pth\", ImagebindEmbedsDataset(\"data/sensor_embeddings_muscle\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\"),\n",
    "    # (\"body only\", \"model_saves/language_decoder/AL_6826stage1_body.pth\", ImagebindEmbedsDataset(\"data/sensor_embeddings_body\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\"),\n",
    "\n",
    "    # (\"VideoLLaMA\", \"model_saves/language_decoder/videollama_6826stage1.pth\", ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test.csv\"), \"regular\")\n",
    "]\n",
    "\n",
    "for name, qformer_path, dataset, prompt_type in models_to_evaluate:\n",
    "    print(\"=\" * 50)\n",
    "    print(name)\n",
    "    load_qformer(model, qformer_path)\n",
    "    outputs, captions_list = generate_eval_sample(dataset, prompt_type)\n",
    "    print(outputs[0])\n",
    "    calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-finetuned VideoLLaMA eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.caption_metrics import calculate_metrics_new\n",
    "from dataset.ImagebindEmbedsDataset import reworded_captions\n",
    "\n",
    "activity_labels = list(caption.replace(\"</s>\", \"\") for caption in reworded_captions.values())\n",
    "vanilla_prompt = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that can understand videos. The user will give you a video and ask you for a caption. You should choose one of the following captions to output:\\n\"\n",
    "for label in activity_labels:\n",
    "    vanilla_prompt += f\"- {label}\\n\"\n",
    "vanilla_prompt += f\"You should output a single caption and nothing more. You don't need to explain.\\n<</SYS>>\\n\\nOpen your eyes and imagine you see: {'<ImageHere>' * 8}. Give me a caption. Only output the caption; do not output anything else. [/INST]\\n\"\n",
    "print(vanilla_prompt)\n",
    "\n",
    "outputs, captions_list = generate_eval_sample(ImagebindEmbedsDataset(\"data/imagebind_targets\", \"data/actionsense_processed\", \"data/test_random_8_old.csv\"), prompt=vanilla_prompt, num=20)\n",
    "outputs = [output.replace(\"<s>\",\"\").replace(\"\\n\",\"\") for output in outputs]\n",
    "outputs = [\" \".join(output.split(\" \")[:30]) for output in outputs] # if response is too long, metrics will error; choose first 30 words\n",
    "print(outputs[0:5])\n",
    "calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_qformer(model, \"model_saves/language_decoder/regular.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "import csv\n",
    "from dataset.ImagebindEmbedsDataset import reworded_captions\n",
    "\n",
    "outputs = []\n",
    "captions_list = []\n",
    "# for imagebind_embeds, captions in tqdm(islice(DataLoader(dataset_test, batch_size=1, shuffle=True), 5)):\n",
    "for imagebind_embeds, captions in tqdm(DataLoader(dataset_test, batch_size=1, shuffle=True)):\n",
    "    imagebind_embeds = imagebind_embeds.to(device)\n",
    "    outputs.append(model.generate(imagebind_embeds).replace(\"<s> \", \"\").replace(\"</s>\", \"\"))\n",
    "    captions_list.append(captions[0].replace(\"<s> \", \"\").replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in islice(zip(outputs, captions_list), 5):\n",
    "    print(x)\n",
    "print(\"Accuracy: \", sum(output == caption for output, caption in zip(outputs, captions_list)))\n",
    "\n",
    "recognized_captions = list(caption.replace(\"</s>\", \"\") for caption in reworded_captions.values()) + [\"other\"]\n",
    "adjusted_outputs = [(output if output in recognized_captions else \"other\") for output in outputs]\n",
    "print(adjusted_outputs.count(\"other\"))\n",
    "confusion_matrix = [[0 for i in range(len(recognized_captions))] for j in range(len(recognized_captions))]\n",
    "for output, caption in zip(adjusted_outputs, captions_list):\n",
    "    confusion_matrix[recognized_captions.index(caption)][recognized_captions.index(output)] += 1\n",
    "\n",
    "pprint(confusion_matrix)\n",
    "\n",
    "with open('language_decoder/eval_results/confusion_matrix_big.csv', 'w', newline='') as file:\n",
    "   csv.writer(file).writerows(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_groups = [\n",
    "    [\n",
    "        'A person is cleaning a pan with a sponge.',\n",
    "        'A person is cleaning a pan with a towel.',\n",
    "        'A person is cleaning a plate with a sponge.',\n",
    "        'A person is cleaning a plate with a towel.',\n",
    "        'A person is clearing the cutting board.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is getting plates, bowls, mugs, glasses, and utensils from the cabinets.',\n",
    "        'A person is retrieving items from the refrigerator, cabinets, and drawers.',\n",
    "        'A person is replacing items in the refrigerator, cabinets, and drawers.',\n",
    "        'A person is loading the dishwasher with plates, bowls, mugs, glasses, and utensils.',\n",
    "        'A person is unloading the plates, bowls, mugs, glasses, and utensils from the dishwasher.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is putting plates and bowls onto the table.',\n",
    "        'A person is setting the table with plates, bowls, mugs, glasses, and utensils.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is opening a jar of almond butter.',\n",
    "        'A person is opening and closing a jar of almond butter.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is peeling a cucumber.',\n",
    "        'A person is peeling a potato.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is slicing a cucumber.',\n",
    "        'A person is slicing a potato.',\n",
    "        'A person is slicing the bread.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is spreading almond butter on a bread slice.',\n",
    "        'A person is spreading jelly on a bread slice.',\n",
    "    ],\n",
    "    [\n",
    "        'A person is pouring water from a pitcher into a glass.',\n",
    "    ],\n",
    "    [\n",
    "        'other'\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group(x):\n",
    "    for i in range(len(caption_groups)):\n",
    "        if x in caption_groups[i]:\n",
    "            return i\n",
    "\n",
    "group_confusion_matrix = [[0 for i in range(len(caption_groups))] for j in range(len(caption_groups))]\n",
    "for output, caption in zip(adjusted_outputs, captions_list):\n",
    "    group_confusion_matrix[find_group(caption)][find_group(output)] += 1\n",
    "pprint(group_confusion_matrix)\n",
    "\n",
    "with open('language_decoder/eval_results/group_confusion_matrix_big.csv', 'w', newline='') as file:\n",
    "   csv.writer(file).writerows(group_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('videollama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12fc2016cfa40a85c17bb5faa80a8240ee193b1fac15980e1262bc37ef6d15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
