{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# pretend to be in root dir\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dataset.ImagebindEmbedsDataset import reworded_captions\n",
    "import anthropic\n",
    "\n",
    "anthropic_client = anthropic.Anthropic(api_key=\"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\")\n",
    "\n",
    "activity_labels = list(caption.replace(\"</s>\", \"\") for caption in reworded_captions.values())\n",
    "prompt = f\"Help me give a caption for this scene. Based on the image, choose a caption from the following list:\\n\"\n",
    "for label in activity_labels:\n",
    "    prompt += f\"- {label}\\n\"\n",
    "prompt += f\"You should output a single caption and nothing more. You don't need to explain.\"\n",
    "\n",
    "def openai_completion(base64_image):\n",
    "    api_key = \"sk-proj-ylR2RB6JlOoeSZLN5K04T3BlbkFJGN90KmxK6CM8t73kdfYh\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\" }\n",
    "                }\n",
    "            ]\n",
    "        }],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    try:\n",
    "        output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except:\n",
    "        print(\"error:\", response.json())\n",
    "    return output\n",
    "\n",
    "def claude_completion(base64_image):\n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": base64_image,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return ' '.join(block.text for block in message.content if isinstance(message.content, list) and block.type == 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A person is retrieving items from the refrigerator, cabinets, and drawers., A person is retrieving items from the refrigerator, cabinets, and drawers.</s>:   7%|▋         | 12/163 [00:49<11:20,  4.51s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping to wait out api limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A person is retrieving items from the refrigerator, cabinets, and drawers., A person is retrieving items from the refrigerator, cabinets, and drawers.</s>:  23%|██▎       | 38/163 [03:06<09:08,  4.39s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping to wait out api limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I apologize, but none of the provided captions accurately describe the scene shown in the image. The image appears to depict the interior of a vehicle, focusing on a person's hands and legs. I do not feel comfortable speculating further about the nature of the activity being captured without more context., A person is replacing items in the refrigerator, cabinets, and drawers.</s>:  57%|█████▋    | 93/163 [08:07<05:51,  5.02s/it]                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping to wait out api limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A person is slicing a potato., A person is slicing the bread.</s>:  77%|███████▋  | 125/163 [11:06<02:38,  4.16s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping to wait out api limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A person is retrieving items from the refrigerator, cabinets, and drawers., A person is clearing the cutting board.</s>:  99%|█████████▉| 161/163 [14:05<00:08,  4.04s/it]                                                                                                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping to wait out api limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A person is cleaning a plate with a sponge., A person is replacing items in the refrigerator, cabinets, and drawers.</s>: 100%|██████████| 163/163 [14:42<00:00,  5.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO: caption getting is all wrong, try to bootstrap one of the datasets if possible\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import cv2\n",
    "import requests\n",
    "import os\n",
    "from dataset.ImagebindEmbedsDataset import ImagebindEmbedsDataset, reworded_captions\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "dirs = [\n",
    "    (\"data/videos_processed/2022-06-07_18-11-37_S00_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S00\"),\n",
    "    (\"data/videos_processed/2022-06-13_21-48-24_S02_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S02_1\"),\n",
    "    (\"data/videos_processed/2022-06-13_22-35-11_S02_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S02_2\"),\n",
    "    (\"data/videos_processed/2022-06-13_23-22-44_S02_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S02_3\"),\n",
    "    (\"data/videos_processed/2022-06-14_13-52-57_S03_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S03\"),\n",
    "    (\"data/videos_processed/2022-06-14_16-38-43_S04_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S04\"),\n",
    "    (\"data/videos_processed/2022-06-14_20-46-12_S05_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S05\"),\n",
    "    (\"data/videos_processed/2022-07-12_15-08-08_S06_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S06\"),\n",
    "    (\"data/videos_processed/2022-07-13_11-02-03_S07_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S07\"),\n",
    "    (\"data/videos_processed/2022-07-13_14-15-26_S08_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S08\"),\n",
    "    (\"data/videos_processed/2022-07-14_09-59-00_S09_eye-tracking-video-worldGaze_frame\", \"data/imagebind_targets/S09\"),\n",
    "]\n",
    "\n",
    "\n",
    "dataset = ImagebindEmbedsDataset(\"data/sensor_embeddings\", \"data/actionsense_processed\", \"data/val_random_8.csv\", include_filepaths=True)\n",
    "outputs = []\n",
    "captions_list = []\n",
    "\n",
    "for _, caption, filepath in (pbar := tqdm(dataset)):\n",
    "    subdir, idx = filepath.split(\"/\")\n",
    "    video_dir = next(dir[0] for dir in dirs if dir[1].split(\"/\")[-1].startswith(subdir))\n",
    "    video_path = f\"{video_dir}/{idx}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        print(f\"Failed to read video: {video_path}\")\n",
    "        continue\n",
    "    frame_path = \"frame.jpg\"\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    with open(frame_path, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    try:\n",
    "        output = claude_completion(base64_image)\n",
    "    except:\n",
    "        print(\"sleeping to wait out api limit\")\n",
    "        time.sleep(30)\n",
    "        continue # bug: will skip some things, not completely accurate\n",
    "\n",
    "    outputs.append(output)\n",
    "    captions_list.append(caption)\n",
    "    pbar.set_description(f\"{output}, {caption}\")\n",
    "\n",
    "    os.remove(frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is cleaning a plate with a sponge. A person is cleaning a pan with a sponge.\n",
      "{'testlen': 2140, 'reflen': 1525, 'guess': [2140, 1982, 1824, 1666], 'correct': [889, 611, 422, 260]}\n",
      "ratio: 1.4032786885236699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/ubuntu/miniconda3/envs/videollama/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [5.979 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 11.62 s\n",
      "{'BLEU-1': 0.4154205607474694, 'BLEU-2': 0.3578596837978243, 'BLEU-3': 0.3094361542346253, 'BLEU-4': 0.26076714904012954, 'ROUGE': 0.5892234043395794, 'METEOR': 0.2851693840035176, 'CIDER': 2.9523462201447384, 'SPICE': 0.44006547366768145}\n",
      "accuracy:  36\n",
      "number of non-matching captions:  20\n"
     ]
    }
   ],
   "source": [
    "from evaluation.caption_metrics import calculate_metrics_new\n",
    "\n",
    "captions_list = [caption.replace(\"</s>\", \"\") for caption in captions_list]\n",
    "print(outputs[0], captions_list[0])\n",
    "calculate_metrics_new(outputs, captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is cleaning a plate with a sponge. A person is cleaning a pan with a sponge.\n",
      "138 138\n",
      "{'testlen': 1266, 'reflen': 1301, 'guess': [1266, 1128, 990, 852], 'correct': [837, 600, 420, 260]}\n",
      "ratio: 0.973097617216777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/ubuntu/miniconda3/envs/videollama/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.061 s\n",
      "{'BLEU-1': 0.6431098935800265, 'BLEU-2': 0.5768466701643834, 'BLEU-3': 0.5159125644944473, 'BLEU-4': 0.4493284075229957, 'ROUGE': 0.6568089092311037, 'METEOR': 0.3368640867285706, 'CIDER': 3.385000021370796, 'SPICE': 0.48791527100350635}\n",
      "accuracy:  36\n",
      "number of non-matching captions:  0\n"
     ]
    }
   ],
   "source": [
    "filtered_outputs = []\n",
    "filtered_captions_list = []\n",
    "for output, caption in zip(outputs, captions_list):\n",
    "    if output in activity_labels:\n",
    "        filtered_outputs.append(output)\n",
    "        filtered_captions_list.append(caption)\n",
    "print(filtered_outputs[0], filtered_captions_list[0])\n",
    "print(len(filtered_outputs), len(filtered_captions_list))\n",
    "calculate_metrics_new(filtered_outputs, filtered_captions_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('videollama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12fc2016cfa40a85c17bb5faa80a8240ee193b1fac15980e1262bc37ef6d15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
